{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 用数学表达式讲解梯度\n",
    "\n",
    "当然可以！我们可以通过数学表达式来讲解梯度的概念以及如何计算梯度。\n",
    "\n",
    "### 梯度的定义\n",
    "\n",
    "在数学中，梯度是一个向量，表示函数在某一点的变化率或方向。具体来说，对于一个多变量函数，梯度向量指向函数值上升最快的方向。\n",
    "\n",
    "#### 单变量函数的梯度\n",
    "\n",
    "对于一个单变量函数 \\( f(x) \\)，梯度就是该函数的导数：\n",
    "\n",
    "$$\n",
    "\\frac{d f(x)}{d x}\n",
    "$$\n",
    "这是函数 \\( f \\) 在点 \\( x \\) 处的变化率。如果 \\( f(x) = x^2 \\)，则：\n",
    "\n",
    "$$\n",
    "\\frac{d f(x)}{d x} = \\frac{d}{d x} (x^2) = 2x\n",
    "$$\n",
    "\n",
    "\n",
    "#### 多变量函数的梯度\n",
    "\n",
    "对于一个多变量函数 \\( f(x_1, x_2, \\ldots, x_n) \\)，梯度是一个向量，其中每个分量是函数对相应变量的偏导数。梯度向量表示函数在点 \\( (x_1, x_2, \\ldots, x_n) \\) 处的最陡上升方向。\n",
    "\n",
    "- **梯度向量**：\n",
    "  $$\n",
    "  \\nabla f = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]\n",
    "  $$\n",
    "  其中：\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x_i}\n",
    "  $$\n",
    "  是函数 \\( f \\) 对变量 \\( x_i \\) 的偏导数。\n",
    "\n",
    "### 梯度计算的例子\n",
    "\n",
    "#### 示例 1：简单的多变量函数\n",
    "\n",
    "考虑函数：\n",
    "\\[ f(x, y) = x^2 + y^2 \\]\n",
    "\n",
    "- **计算梯度**：\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} (x^2 + y^2) = 2x\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} (x^2 + y^2) = 2y\n",
    "  $$\n",
    "  \n",
    "  因此梯度向量为：\n",
    "  $$\n",
    "  \\nabla f = \\left[ 2x, 2y \\right]\n",
    "  $$\n",
    "  如果 \\( x = 1 \\) 和 \\( y = 2 \\)，则：\n",
    "  $$\n",
    "  \\nabla f = \\left[ 2 \\times 1, 2 \\times 2 \\right] = \\left[ 2, 4 \\right]\n",
    "  $$\n",
    "  \n",
    "\n",
    "#### 示例 2：张量操作中的梯度\n",
    "\n",
    "考虑张量操作中的梯度计算：\n",
    "\n",
    "- **张量 \\( x \\)**：\n",
    "  $$\n",
    "  x = [x_0, x_1, x_2, x_3]\n",
    "  $$\n",
    "  \n",
    "  \n",
    "- **计算 \\( y \\)**：\n",
    "  $$\n",
    "  y = x^2 \\text{（逐元素平方）}\n",
    "  $$\n",
    "  \n",
    "  \n",
    "- **计算 \\( z \\)**：\n",
    "  $$\n",
    "  z = u \\cdot x \\text{（其中} u = x^2 \\text{）}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  z = (x^2) \\cdot x = x^3\n",
    "  $$\n",
    "  \n",
    "  \n",
    "  \n",
    "- **计算 \\( z \\) 对 \\( x \\) 的梯度**：\n",
    "  $$\n",
    "  \\frac{\\partial z}{\\partial x_i} = \\frac{\\partial (x_i^3)}{\\partial x_i} = 3x_i^2\n",
    "  $$\n",
    "  \n",
    "\n",
    "### PyTorch 中的梯度计算\n",
    "\n",
    "在 PyTorch 中，梯度是通过计算图和自动微分来完成的。当调用 `backward()` 方法时，PyTorch 会自动计算所有张量的梯度。\n",
    "\n",
    "#### 代码示例\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 创建张量 x\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 计算 y = x^2\n",
    "y = x * x\n",
    "\n",
    "# 计算 z = sum(y)\n",
    "z = y.sum()\n",
    "\n",
    "# 执行反向传播\n",
    "z.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(x.grad)  # 输出: tensor([2., 4., 6.])\n",
    "```\n",
    "\n",
    "- **解释**：\n",
    "  - `y = x * x` 计算了每个元素的平方。\n",
    "  - `z = y.sum()` 计算了所有元素的总和。\n",
    "  - `z.backward()` 计算 `z` 对 `x` 的梯度。\n",
    "  - 对于每个元素 \\( x_i \\)，梯度是 \\( 2x_i \\)，这与我们手动计算的结果一致。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **梯度**：表示函数在某一点上升最快的方向的变化率。\n",
    "- **单变量函数**：梯度是导数。\n",
    "- **多变量函数**：梯度是偏导数的向量。\n",
    "- **PyTorch 中的梯度计算**：通过计算图和自动微分机制计算，`backward()` 方法用于执行反向传播并计算梯度。"
   ],
   "id": "9c1054ac9caed762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 原版\n",
    "\n",
    "作为一个演示例子，假设我们想对函数 y = 2·xT·x\n",
    "关于列向量 x\n",
    "求导。 首先，我们创建变量x并为其分配一个初始值。"
   ],
   "id": "c9420f1607c1230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "id": "d6d9c575eff87555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x.requires_grad_(True)  # 设置张量 x 的 requires_grad 属性为 True。这意味着该张量在后续的操作中将会被 PyTorch 用于计算梯度，并且将会被追踪以构建计算图。\n",
    "x.grad  # x.grad 是 PyTorch 中张量 x 的一个属性，用于存储计算得到的梯度。默认值是None"
   ],
   "id": "d9a667d14825fdfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 设y:\n",
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ],
   "id": "346f24ae83fc016d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y.backward()  # 会计算当前张量 y 相对于其所有输入张量(即x)的梯度，并将这些梯度存储在这些输入张量(即x)的 .grad 属性中。\n",
    "x.grad"
   ],
   "id": "4f01eacda9d9d487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "即函数\n",
    "$$\n",
    "y = 2 \\times \\text{dot}(x, x) = 2 \\times \\sum_{i} x_i^2\n",
    "$$\n",
    "的偏导函数为\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( 2 \\times \\sum_{i} x_i^2 \\right) = 4 x_j\n",
    "$$\n",
    "\n",
    "所以，x = [0., 1., 2., 3.] 的梯度是 x.grad = 4x = [0.,  4.,  8., 12.] 。"
   ],
   "id": "a0adf506a7e87a31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 现在计算x的另一个函数。\n",
    "\n",
    "x.grad.zero_()  # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "y = x.sum()  # 计算张量 x 的所有元素的总和，并将结果存储在 y 中。\n",
    "y.backward()  # 会计算当前张量 y 相对于其所有输入张量(即x)的梯度，并将这些梯度存储在这些输入张量(即x)的 .grad 属性中。\n",
    "x, y, x.grad"
   ],
   "id": "3367660aaa887c58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "因为函数为\n",
    "$$\n",
    "y = x_0 + x_1 + x_2 + x_3\n",
    "$$\n",
    "所以偏导函数为\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_0} = \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial x_3} = 1\n",
    "$$\n"
   ],
   "id": "6fe1acd15ebf5443"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 非标量变量的反向传播",
   "id": "5cf945bae51f1906"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 分离计算\n",
    "\n",
    "有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "\n",
    "这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是 z = x * x * x 关于x的偏导数。"
   ],
   "id": "219ecc8f1338c2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 将 x 的梯度置为零\n",
    "x.grad.zero_()\n",
    "\n",
    "# 计算 y = x * x\n",
    "y = x * x  # 元素级别的平方\n",
    "\n",
    "# 通过 .detach() 创建一个新的张量 u，不会计算梯度\n",
    "u = y.detach()\n",
    "\n",
    "# 计算 z = u * x\n",
    "z = u * x\n",
    "\n",
    "# 对 z 的总和进行反向传播\n",
    "z.sum().backward()\n",
    "\n",
    "x.grad == u\n"
   ],
   "id": "92fe2a2cbe71b1ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "id": "e6ed434b292a40e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Python控制流的梯度计算",
   "id": "bf708574154ee6e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ],
   "id": "5d085dda5f9a3a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ],
   "id": "98aed2ef060bbe07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a.grad",
   "id": "3995881dd0911db9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "d / a",
   "id": "c5f98e33b69ba9ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
