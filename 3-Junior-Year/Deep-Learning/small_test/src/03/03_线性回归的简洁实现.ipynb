{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 生成数据集",
   "id": "b8f694ba757bd208"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "num_inputs = 2  # 是输入特征的数量，这里是 2。\n",
    "num_examples = 1000  # 是样本的数量，这里是 1000。\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. **设置参数**\n",
    "   ```python\n",
    "   num_inputs = 2\n",
    "   num_examples = 1000\n",
    "   true_w = [2, -3.4]\n",
    "   true_b = 4.2\n",
    "   ```\n",
    "   - `num_inputs` 定义了输入特征的数量，这里为 2。\n",
    "   - `num_examples` 定义了样本的数量，这里为 1000。\n",
    "   - `true_w` 是真实的权重，值为 `[2, -3.4]`。\n",
    "   - `true_b` 是真实的偏置，值为 `4.2`。\n",
    "\n",
    "3. **生成特征数据**\n",
    "   ```python\n",
    "   features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "   ```\n",
    "   - 使用 `numpy` 的 `np.random.normal` 函数生成一个形状为 `(1000, 2)` 的矩阵，其元素服从均值为 0，标准差为 1 的正态分布。\n",
    "   - 将生成的 NumPy 数组转换为 PyTorch 的张量（`torch.tensor`），并指定数据类型为 `torch.float`。\n",
    "\n",
    "4. **生成标签数据**\n",
    "   ```python\n",
    "   labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "   ```\n",
    "   - 使用真实的权重和偏置计算每个样本的标签值。\n",
    "   - `features[:, 0]` 获取所有样本的第一个特征，`features[:, 1]` 获取所有样本的第二个特征。\n",
    "   - 计算公式为：`labels = 2 * features[:, 0] + (-3.4) * features[:, 1] + 4.2`。\n",
    "\n",
    "5. **添加噪声**\n",
    "   ```python\n",
    "   labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "   ```\n",
    "   - 使用 `numpy` 的 `np.random.normal` 函数生成一个与 `labels` 形状相同的噪声张量，其元素服从均值为 0，标准差为 0.01 的正态分布。\n",
    "   - 将生成的噪声转换为 PyTorch 张量并添加到 `labels` 中，以模拟真实数据中的噪声。\n",
    "\n",
    "### 总结\n",
    "\n",
    "这段代码生成了一个简单的线性回归数据集，包括两个特征和一个带有噪声的标签。生成的数据集可以用于训练和测试线性回归模型。在这里：\n",
    "\n",
    "- `features` 是输入特征矩阵，形状为 `(1000, 2)`。\n",
    "- `labels` 是目标标签向量，形状为 `(1000,)`，其值是根据线性模型（加上噪声）计算得出的。"
   ],
   "id": "f145f80ddf72b713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 读取数据",
   "id": "51accd26decc2f37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ],
   "id": "467c3403879c6863",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **导入模块**\n",
    "   ```python\n",
    "   import torch.utils.data as Data\n",
    "   ```\n",
    "   - 导入 PyTorch 的数据工具包，主要用于处理数据集和数据加载。\n",
    "\n",
    "2. **设置批量大小**\n",
    "   ```python\n",
    "   batch_size = 10\n",
    "   ```\n",
    "   - 定义每个批量的数据量，这里设置为 10。\n",
    "\n",
    "3. **创建数据集**\n",
    "   ```python\n",
    "   dataset = Data.TensorDataset(features, labels)\n",
    "   ```\n",
    "   - `Data.TensorDataset` 是一个简单的数据集封装器，用于将特征和标签组合在一起。\n",
    "   - `features` 和 `labels` 分别是之前生成的特征张量和标签张量。\n",
    "   - `dataset` 是一个包含输入特征和目标标签的数据集对象。\n",
    "\n",
    "4. **创建数据加载器**\n",
    "   ```python\n",
    "   data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "   ```\n",
    "   - `Data.DataLoader` 是 PyTorch 提供的一个数据加载器，能够以小批量的方式加载数据。\n",
    "   - 参数：\n",
    "     - `dataset`: 数据集对象。\n",
    "     - `batch_size`: 每个批量的数据量，这里设置为 10。\n",
    "     - `shuffle`: 是否在每个 epoch 开始时打乱数据，这里设置为 `True`，表示打乱数据。\n",
    "\n",
    "### 使用\n",
    "\n",
    "在训练过程中，可以使用 `data_iter` 来遍历数据集。每次迭代时，它将返回一个包含一小批特征和标签的元组。示例如下：\n",
    "\n",
    "```python\n",
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break  # 只打印一个批量的数据\n",
    "```\n",
    "\n",
    "这样可以确保每次训练时都能以小批量的方式读取数据，并且每个 epoch 数据的顺序都是随机的，有助于提高模型的泛化能力。"
   ],
   "id": "f2be49ce29fe7d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    # print(data_iter)\n",
    "    # break"
   ],
   "id": "507b46d73002735a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 定义模型\n",
    "\n",
    "在上一节从零开始的实现中，我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更繁琐。其实，PyTorch提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用PyTorch更简洁地定义线性回归。\n",
    "\n",
    "首先，导入torch.nn模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了autograd，而nn就是利用autograd来定义模型。nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络/层。一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法。下面先来看看如何用nn.Module实现一个线性回归模型。\n",
    "\n",
    "==========================\n",
    "\n",
    "\n",
    "当我们在 3.2节中实现线性回归时， 我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。 但是，如果模型变得更加复杂，且当我们几乎每天都需要实现模型时，自然会想简化这个过程。 这种情况类似于为自己的博客从零开始编写网页。 做一两次是有益的，但如果每个新博客就需要工程师花一个月的时间重新开始编写网页，那并不高效。\n",
    "\n",
    "对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。 但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉“标准的流水线”。\n",
    "\n",
    "回顾 图3.1.2中的单层网络架构， 这一单层被称为全连接层（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。\n",
    "\n",
    "在PyTorch中，全连接层在Linear类中定义。 值得注意的是，我们将两个参数传递到nn.Linear中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。"
   ],
   "id": "47781f38f81d0282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    # 此处还可以传入其他层\n",
    ")\n",
    "\n",
    "print(net)"
   ],
   "id": "703e380288d36dc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "线性层的数学公式可以用来解释其操作。具体来说，对于 `nn.Linear(num_inputs, 1)` 定义的线性层，其数学表达如下：\n",
    "\n",
    "### 数学公式解释\n",
    "\n",
    "#### 线性层操作\n",
    "\n",
    "对于输入特征矩阵 \\( X \\) 和线性层的权重矩阵 \\( W \\) 和偏置项 \\( b \\)，线性层的输出 \\( y \\) 可以表示为：\n",
    "\n",
    "$$ y = X \\cdot W^T + b $$\n",
    "\n",
    "其中：\n",
    "- \\( X \\) 是输入特征矩阵，维度为 \\((N, \\text{num\\_inputs})\\)，其中 \\(N\\) 是样本数量，\\(\\text{num\\_inputs}\\) 是每个样本的特征数量。\n",
    "- \\( W \\) 是权重矩阵，维度为 \\((\\text{num\\_inputs}, 1)\\)，其中 \\(1\\) 是输出特征的数量。\n",
    "- \\( b \\) 是偏置项，通常是一个标量（或者在 PyTorch 中表示为一个维度为 \\(1\\) 的向量）。\n",
    "- \\( y \\) 是输出特征矩阵，维度为 \\((N, 1)\\)。\n",
    "\n",
    "#### 解释\n",
    "\n",
    "1. **权重矩阵 \\( W \\)**:\n",
    "   - 在 `nn.Linear(num_inputs, 1)` 中，权重矩阵 \\( W \\) 的维度是 \\(\\text{num\\_inputs} \\times 1\\)。这是因为我们从 \\(\\text{num\\_inputs}\\) 个输入特征映射到 1 个输出特征。矩阵的转置 \\( W^T \\) 的维度是 \\(1 \\times \\text{num\\_inputs}\\)，用于和输入矩阵 \\( X \\) 进行矩阵乘法。\n",
    "\n",
    "2. **偏置 \\( b \\)**:\n",
    "   - 偏置 \\( b \\) 是一个标量，表示输出特征的每个维度的偏移量。在实际计算中，偏置会被广播到输出的每个样本。\n",
    "\n",
    "3. **前向传播计算**:\n",
    "   - 对于每个输入样本 \\( x \\)（一个 \\(\\text{num\\_inputs}\\)-维向量），线性层的计算过程是：\n",
    "     $$ y = x \\cdot W + b $$\n",
    "   - 这里，\\( x \\cdot W \\) 表示输入向量与权重矩阵的矩阵乘法，结果是一个标量（因为 \\( W \\) 的维度是 \\(\\text{num\\_inputs} \\times 1\\)）。最后加上偏置 \\( b \\) 产生最终的输出 \\( y \\)。\n",
    "\n",
    "#### 详细步骤\n",
    "\n",
    "1. **矩阵乘法**:\n",
    "   - 将输入特征 \\( X \\) 与权重矩阵 \\( W \\) 相乘，计算结果是一个 \\((N, 1)\\) 维的矩阵。每个样本的特征向量通过权重矩阵变换成一个标量。\n",
    "\n",
    "2. **加上偏置**:\n",
    "   - 对于每个样本的输出，偏置 \\( b \\) 被加到结果上。这一步调整了输出的值，使其符合数据的实际情况。\n",
    "\n",
    "### 总结\n",
    "\n",
    "在 `nn.Linear(num_inputs, 1)` 定义的线性层中，网络的每一层都执行线性变换，即将输入特征通过权重矩阵和偏置项进行变换。这种变换的数学表示是：\n",
    "\n",
    "$$ y = X \\cdot W^T + b $$\n",
    "\n",
    "这个公式表示了如何从输入特征 \\( X \\) 通过线性变换得到输出 \\( y \\)。"
   ],
   "id": "38f33f1b957431a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "解释\n",
    "\n",
    "Sequential:\n",
    "\n",
    "Sequential 是一个容器，按顺序包含了一系列的层（modules）。在这个例子中，它只包含一个线性层。\n",
    "(0): Linear(in_features=2, out_features=1, bias=True):\n",
    "\n",
    "(0) 表示这是 Sequential 容器中的第一个（也是唯一一个）层。层的编号是从 0 开始的。\n",
    "Linear(in_features=2, out_features=1, bias=True) 描述了一个线性层，其具体参数如下：\n",
    "in_features=2: 输入特征的维度是 2。这意味着每个输入样本有 2 个特征。\n",
    "out_features=1: 输出特征的维度是 1。这个线性层将 2 个输入特征映射到 1 个输出特征。\n",
    "bias=True: 表示这个线性层包含偏置项。偏置项是一个额外的参数，用于调整输出。"
   ],
   "id": "4297c4641a7b13df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 初始化模型参数",
   "id": "98dcd5b06326d6c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ],
   "id": "f03cdd320abf1bf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "详细说明\n",
    "\n",
    "net[0].weight.data.normal_(0, 0.01):\n",
    "\n",
    "作用: 用正态分布（均值为 0，标准差为 0.01）初始化第一个线性层的权重。\n",
    "解释:\n",
    "net[0] 指的是 Sequential 容器中的第一个层，这里是 nn.Linear。\n",
    "weight.data 访问层的权重数据。\n",
    "normal_(0, 0.01) 是一个原地操作，用于将权重数据初始化为均值为 0，标准差为 0.01 的正态分布。\n",
    "\n",
    "\n",
    "net[0].bias.data.fill_(0):\n",
    "\n",
    "作用: 将第一个线性层的偏置初始化为 0。\n",
    "解释:\n",
    "bias.data 访问层的偏置数据。\n",
    "fill_(0) 是一个原地操作，将所有偏置值设置为 0。"
   ],
   "id": "82b3dea9915b7503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义损失函数",
   "id": "3282ac8e752c37a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loss = nn.MSELoss()",
   "id": "2a327b795dd391a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`nn.MSELoss` 是 PyTorch 中的一个损失函数，用于计算均方误差（Mean Squared Error，MSE）。均方误差是回归任务中常用的损失函数，它用于衡量预测值与真实值之间的差距。\n",
    "\n",
    "### 详细解释\n",
    "\n",
    "#### 1. **均方误差（MSE）**\n",
    "\n",
    "均方误差的数学公式是：\n",
    "\n",
    "$$\n",
    "\\text{MSE}(y_{\\text{hat}}, y) = \\frac{1}{N} \\sum_{i=1}^N (y_{\\text{hat}, i} - y_i)^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- \\( y_{\\text{hat}} \\) 是模型的预测值。\n",
    "- \\( y \\) 是真实值。\n",
    "- \\( N \\) 是样本数量。\n"
   ],
   "id": "a6bee457db77cfe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义优化算法",
   "id": "90fad4ee9c49a200"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer = torch.optim.SGD(net.parameters(), lr=0.03)",
   "id": "4c0b33e7a9d3741a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这行代码创建了一个随机梯度下降（SGD）优化器，用于训练 PyTorch 模型。以下是对这行代码的详细解释：\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "```python\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "```\n",
    "\n",
    "### 详细说明\n",
    "\n",
    "1. **`torch.optim.SGD`**:\n",
    "   - `torch.optim.SGD` 是 PyTorch 中实现随机梯度下降（SGD）优化算法的类。SGD 是一种常用的优化算法，用于通过迭代更新模型参数以最小化损失函数。\n",
    "\n",
    "2. **`net.parameters()`**:\n",
    "   - `net` 是一个 PyTorch 模型实例（例如，`nn.Sequential` 或自定义的 `nn.Module` 类）。\n",
    "   - `net.parameters()` 返回模型的所有可训练参数（权重和偏置）。这些参数是 SGD 优化器要更新的对象。\n",
    "\n",
    "3. **`lr=0.03`**:\n",
    "   - `lr` 是学习率（learning rate），它控制了每次参数更新的步长大小。在这个例子中，学习率设为 0.03。学习率是优化算法中的一个重要超参数，影响训练的收敛速度和稳定性。\n",
    "\n",
    "### 随机梯度下降（SGD）\n",
    "\n",
    "**随机梯度下降** 是一种优化算法，用于更新模型的权重。其基本思想是通过计算损失函数的梯度，并按梯度方向更新参数。其更新规则为：\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- \\(\\theta\\) 是模型的参数（例如权重和偏置）。\n",
    "- \\(\\eta\\) 是学习率（learning rate）。\n",
    "- \\(\\nabla_\\theta J(\\theta)\\) 是损失函数 \\(J\\) 相对于参数 \\(\\theta\\) 的梯度。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`torch.optim.SGD`** 是一个实现随机梯度下降的优化器类。\n",
    "- **`net.parameters()`** 提供了需要优化的模型参数。\n",
    "- **`lr=0.03`** 设置了优化器的学习率，控制了参数更新的步长。\n",
    "- 在训练过程中，优化器通过迭代更新模型参数，以最小化损失函数。"
   ],
   "id": "e4a4070464fae63b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "8da6d62c5471fb65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        y = y.view(-1, 1)  # 调整标签形状\n",
    "        l = loss(net(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels.view(-1, 1))  # 调整标签形状\n",
    "    print(f'epoch {epoch + 1}, loss {l.item():f}')"
   ],
   "id": "b925afccd02a5e78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w = net[0].weight.data\n",
    "# 转换 true_w 为 Tensor\n",
    "true_w_tensor = torch.tensor(true_w, dtype=torch.float)\n",
    "print('w的估计误差：', true_w_tensor - w.view(-1))\n",
    "b = net[0].bias.data\n",
    "true_b_tensor = torch.tensor(true_b, dtype=torch.float)\n",
    "print('b的估计误差：', true_b_tensor - b)"
   ],
   "id": "64bed6f75c833e3e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
